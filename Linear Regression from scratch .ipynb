{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e93413d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31a2cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Import Libraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95102bf",
   "metadata": {},
   "source": [
    "# Linear Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04ad5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression: #Class For Linear Regression\n",
    "    \n",
    "    def __init__(self,x):\n",
    "        self.theta=np.random.randn(x.shape[1]+1)  #Generate Random Weights\n",
    "        \n",
    "    def g_descent(self,x,y):\n",
    "        deri=self.deri(x,y)  #Get Derivative\n",
    "        self.theta=self.theta-self.learning_rate*deri.T #Calculate New Theta\n",
    "        return self.theta\n",
    "    \n",
    "    def hypo(self,x):\n",
    "        x=np.column_stack((x,np.ones((x.shape[0]))))  #Append One extra Column at the end of x\n",
    "        return np.dot(x,self.theta.T)  #Calculate hypothesis\n",
    "    \n",
    "    def deri(self,x,y):\n",
    "        hx=self.hypo(x)\n",
    "        return np.dot(x.T,(y-hx))/len(y)  #Calculate Derivative\n",
    "    \n",
    "    def cost(self,x,y):\n",
    "        hx=self.hypo(x)  #Calculate cost to anylize loss\n",
    "        return np.mean(np.square(y-hx))\n",
    "    \n",
    "    def getweights(self):\n",
    "        return self.theta #Get Current Weights \n",
    "    \n",
    "    def predict(self,x):\n",
    "        return self.hypo(x) #Predict test data\n",
    "    \n",
    "    def train(self,x,y,learning_rate,epochs,print_interval):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.epochs=epochs\n",
    "        if learning_rate is None:   #Default setting the learning rate and print interval\n",
    "            self.learning_rate=0.001\n",
    "        else:\n",
    "            self.learning_rate=learning_rate\n",
    "        if print_interval is None:\n",
    "            self.print_interval=50\n",
    "        else:\n",
    "            self.print_interval=print_interval\n",
    "        previous=np.inf   #sSetting previous loss as infinity \n",
    "        for epoch in range(epochs):\n",
    "            self.g_descent(x,y)  #Set new Theta\n",
    "            if epoch%self.print_interval==0:\n",
    "                current=self.cost(self.x,y)  #Get Loss\n",
    "                print(\"Loss : \",current)\n",
    "            previous=current  #Setting previous loss as current "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f982c71",
   "metadata": {},
   "source": [
    "# Generating Random Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a497044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  31.62627321873208\n",
      "Loss :  31.63616208261353\n",
      "Loss :  31.64605409670092\n",
      "Loss :  31.655949261988354\n",
      "Loss :  31.665847579470288\n",
      "Loss :  31.675749050141455\n",
      "Loss :  31.6856536749969\n",
      "Loss :  31.695561455032\n",
      "Loss :  31.705472391242427\n",
      "Loss :  31.715386484624172\n",
      "Loss :  31.72530373617355\n",
      "Loss :  31.735224146887163\n",
      "Loss :  31.74514771776195\n",
      "Loss :  31.755074449795153\n",
      "Loss :  31.765004343984334\n",
      "Loss :  31.77493740132736\n",
      "Loss :  31.78487362282241\n",
      "Loss :  31.794813009467987\n",
      "Loss :  31.80475556226289\n",
      "Loss :  31.81470128220626\n",
      "Loss :  31.824650170297524\n",
      "Loss :  31.834602227536447\n",
      "Loss :  31.844557454923084\n",
      "Loss :  31.85451585345782\n",
      "Loss :  31.864477424141352\n",
      "Final Weights Are :  [-0.0019413   1.43859089]\n",
      "The Mean Squared Error is : 34.840576272940666\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) #Make Random Data for Training\n",
    "x=2*np.random.rand(100,1)\n",
    "y=4+3*x.flatten()+np.random.randn(100)\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "x=scaler.fit_transform(x) #Fit and transform the data\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42) #Split into Testing And Training\n",
    "\n",
    "model=LinearRegression(x_train)\n",
    "model.train(x_train,y_train,learning_rate=0.00001,epochs=500,print_interval=20) #Train the Model\n",
    "\n",
    "y_pred=model.predict(x_test) #Predict\n",
    "\n",
    "print(\"Final Weights Are : \",model.getweights())  #Print Final Weights\n",
    "\n",
    "error=mean_squared_error(y_test,y_pred) #Calculate Mean Squared Error\n",
    "print('The Mean Squared Error is :',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642fbc5",
   "metadata": {},
   "source": [
    "# Generating Random Data \n",
    "### (x having multiple features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef4f615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  1.5392780042652259\n",
      "Loss :  1.539531371462211\n",
      "Loss :  1.539784817224308\n",
      "Loss :  1.5400383415759804\n",
      "Loss :  1.5402919445416985\n",
      "Loss :  1.5405456261459407\n",
      "Loss :  1.540799386413194\n",
      "Loss :  1.5410532253679519\n",
      "Loss :  1.5413071430347167\n",
      "Loss :  1.5415611394379969\n",
      "Loss :  1.5418152146023094\n",
      "Loss :  1.5420693685521791\n",
      "Loss :  1.5423236013121377\n",
      "Loss :  1.5425779129067259\n",
      "Loss :  1.5428323033604894\n",
      "Loss :  1.5430867726979853\n",
      "Loss :  1.543341320943775\n",
      "Loss :  1.5435959481224297\n",
      "Loss :  1.543850654258527\n",
      "Loss :  1.544105439376653\n",
      "Loss :  1.5443603035014009\n",
      "Loss :  1.5446152466573717\n",
      "Loss :  1.5448702688691742\n",
      "Loss :  1.5451253701614256\n",
      "Loss :  1.5453805505587492\n",
      "Final Weights Are :  [ 0.07347598 -0.67949029]\n",
      "The Mean Squared Error is : 1.1130939771522843\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)  #Make Random Data for Training (Make x having multiple features)\n",
    "X=2*np.random.rand(100,3)\n",
    "\n",
    "y=np.random.randn(100) #Generate labels randomly\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "x=scaler.fit_transform(x) #Fit and transform the data\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42) #Split into Testing And Training\n",
    "\n",
    "model=LinearRegression(x_train)\n",
    "model.train(x_train,y_train,learning_rate=0.00001,epochs=500,print_interval=20) #Train the Model\n",
    "\n",
    "y_pred=model.predict(x_test) #Predict\n",
    "\n",
    "print(\"Final Weights Are : \",model.getweights())  #Print Final Weights\n",
    "\n",
    "error=mean_squared_error(y_test,y_pred) #Calculate Mean Squared Error\n",
    "print('The Mean Squared Error is :',error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebc565",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note:** I have tried my best to provide accurate results in this notebook. However, these results may not be entirely accurate, and contributions or corrections are encouraged. Thank you!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
